{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec82737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fcfe3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df80ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 256\n",
    "CHANNELS=3\n",
    "EPOCHS=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b795d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "# Define transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.ImageFolder(\n",
    "    root=\"PlantVillage\",\n",
    "    transform=data_transforms\n",
    ")\n",
    "\n",
    "# Create DataLoader for batching and shuffling\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4  # You can adjust this based on your system\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a60cf4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 256, 256])\n",
      "[1 0 1 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 2 1 0 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 1 1 0 2 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 0 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 1 1 1 0 0 0 0 1 1 1 2 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 2 1 1 1 1 1 0 0 0 0 0 0 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 2 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 1 0 0 0 1 1 1 0 2 1 0 0 1 0 1 1 1 1 0 0 1 1 1 2 1 2 1 0 0 1 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 0 0 1 0 2 1 2 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 2 0 1 0 1 1 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[2 1 0 1 1 0 0 1 1 2 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 2 1 0 1 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 2 2 1 0 0 1 0 0 0 0 2 0 0 0 0 1 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 2 1 1 0 0 1 1 0 0 0 0 1 2 1 1 0 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 0 1 1 1 0 0 1 0 0 1 2 0 0 0 0 0 0 1 0 1 2 1 1 1 0 0 0 0 0 1 2]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 0 1 1 1 0 0 1 0 0 0 2 0 1 1 1 1 1 0 2 1 2 0 0 0 0 1 0 2 0 1 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 1 1 2 0 1 1 0 0 1 1 0 1 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 1 2 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 2 1 1 0 0 0 0 0 2 0 0 1 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 2 0 1 0 1 0 0 0 0 1 0 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 1 2 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0 2 2 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 2 0 1 1 0 0 1 0 1 0 0 1 0 0 2 1 0 0 1 0 1 1 1 0 0 1 0 2 1 0 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 0 1 2 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 2 0 0 0 1 1 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 1 0 1 1 2 1 1 1 2 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 2 0 1 1 1 1 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 0 1 1 1 1 0 1 1 1 1 2 0 0 0 1 1 0 1 0 1 0 1 2 0 0 1 1 1 1 2 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 0 2 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 2 2]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 0 1 0 1 1 1 0 0 1 0 0 2 0 0 2 1 0 1 0 0 0 1 0 0 1 1 0 0 2 1 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 1 1 0 1 0 0 0 1 1 2 0 2 1 0 0 1 2 1 1 1 0 1 0 1 0 0 0 1 1 1 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[2 0 0 0 1 1 0 1 2 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 1 1 0 1 1 0 2 1 1 1 0 1 0 1 0 0 0 2 0 0 0 0 2 0 0 0 1 0 1 1 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 1 0 1 1 0 0 0 1 0 1 1 2 1 2 1 0 1 1 0 2 0 1 2 1 1 0 0 1 1 1 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 0 0 0 1 2 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 2 0 0 0 1 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 0 2 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 2 1 2 1 1 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 1 1 1 1 1 0 1 1 0 0 1 0 2 0 0 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 1 1 1 1 0 0 1 1 0 1 1 0 1 2 0 1 1 1 0 1 2 0 0 0 0 2 0 1 0 0 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 0 0 2 0 0 1 0 1 1 1 0 1 1 0 0 0 1 1 1 0 0 1 0 2 1 1 1 1 0 0 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 1 0 0 0 2 2 0 0 1 0 0 1 1 0 0 2 1 1 1 0 1 0 0 2 1 0 1 0 1 1 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 0 0 0 2 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 0 0 1 2 1 1 1 0 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 2 0 0 0 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 0 1 0 1 1 1 2 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 1 2 1 1 0 0 1 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 2 2 0 1 2 1 1 1 0 0 2 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 2 1 2 0 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 1 0 1 0 1 0 0 2 0 1 2 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 1 0 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 2 2 1 0 0 1 0 0 1 2 0 0 1 1 0 1 1 1 0 1 0 2 1 1 1 0 1 1 0 0 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 0 2 2 0 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 1 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 2 1 1 1 1 0 0 0 0 0 0 1 0 1 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 2 1 0 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 0 1 1 1 2 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 2 0 1 0 0 1 1 1 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 0 1 0 2 1 1 1 0 1 1 0 1 1 0 0 0 2 1 0 1 0 0 1 1 1 1 0 1 0 1 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 2 0 1 1 0 0 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 1 0 0 0 1 2 2 0 1 2 1 0 0 0 0 0 0 0 1 0 2 1 0 0 0 0 0 0 1 1 2]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 1 0 0 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 2 1 1 1 1 1 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 1 0 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 1 1 1 1 2 1 1 1 1 0 0 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 1 0 0 1 0 1 1 1 2 0 0 0 1 0 2 0 0 0 1 0 0 1 1 0 0 1 1 2 0 1 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[2 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 2 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 1 0 1 0 1 0 2 0 0 0 1 1 1 1 0 0 0 1 0 1 2 0 0 1 0 0 0 0 1 1 2]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 2 1 1 0 0 1 0 0 1 1 1 2 1 1 1 1 0 0 0 0 2 0 0 0 0 1 1 1 1 1 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 1 2 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 2 0 1 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 0 1 1 0 0 0 0 1 2 1 0 0 0 0 1 0 0 0 1 0 1 2 0 0 1 2 0 1 1 0 2]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 0 1 0 0 0 0 0 2 1 1 0 0 0 2 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 2 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 2 1 0 1 2 0 0 1 1 0 0 0 0 1 2 0 0 0 1 0 1 2 0 1 1 0 1 0 2 0 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 0 0 0 1 1 1 1 1 2 1 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 1 0 1 2 2 0 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 0 2 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 1 0 2 0 1 0 0 0 0 0 1 0 0 1]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 2 1 1 0 0 2 0 1 1 1 1 0 0 1 0 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0 1 0 1 1 2 0 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[2 1 1 1 1 1 0 0 0 0 1 0 0 1 0 2 1 2 1 0 0 0 1 1 0 0 1 0 1 1 2 0]\n",
      "torch.Size([32, 3, 256, 256])\n",
      "[2 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 2 2 1 1 0]\n",
      "torch.Size([8, 3, 256, 256])\n",
      "[0 0 2 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "for image_batch, labels_batch in data_loader:\n",
    "    print(image_batch.shape)  # Shape of the image batch\n",
    "    print(labels_batch.numpy())  # Convert labels tensor to NumPy array\n",
    "    #sab images ko mix kr dia ferr 32 32 images ka batch bnaa dia  usme us leaf k hisaab se label kr dia 0 for early blight ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67bf6715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_dir = \"PlantVillage\"\n",
    "class_names = sorted(os.listdir(data_dir))\n",
    "print(class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7552b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3515e40",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'figure'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_batch, labels_batch \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Create a figure with a 3x4 grid of subplots\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(image_batch), \u001b[38;5;241m12\u001b[39m)):  \u001b[38;5;66;03m# Limiting to the first 12 images\u001b[39;00m\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;66;03m# Plot the image\u001b[39;00m\n\u001b[0;32m      6\u001b[0m         ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\Downloads\\floderconda\\Lib\\site-packages\\matplotlib\\_api\\__init__.py:226\u001b[0m, in \u001b[0;36mcaching_module_getattr.<locals>.__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m props:\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m props[name]\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance)\n\u001b[1;32m--> 226\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'figure'"
     ]
    }
   ],
   "source": [
    "for image_batch, labels_batch in data_loader:\n",
    "    # Create a figure with a 3x4 grid of subplots\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(min(len(image_batch), 12)):  # Limiting to the first 12 images\n",
    "        # Plot the image\n",
    "        ax = plt.subplot(3, 4, i + 1)\n",
    "        plt.imshow(np.transpose(image_batch[i], (1, 2, 0)))  # Transpose the image tensor to (H, W, C) for plotting\n",
    "        plt.title(class_names[labels_batch[i].item()])\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e063ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 54\n",
    "train_ds = torch.utils.data.Subset(dataset, range(num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bbfcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_validation = 6\n",
    "val_ds = torch.utils.data.Subset(test_ds, range(num_samples_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e0c735",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 54\n",
    "test_ds = torch.utils.data.Subset(dataset, range(start_index, len(dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a8c2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_validation = 6\n",
    "test_ds = torch.utils.data.Subset(test_ds, range(num_samples_validation, len(test_ds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cc487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "def get_dataset_partitions_pytorch(dataset, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "    \n",
    "    dataset_size = len(dataset)\n",
    "    \n",
    "    if shuffle:\n",
    "        # Create indices for shuffling\n",
    "        indices = torch.randperm(dataset_size)\n",
    "        # Shuffle the dataset based on indices\n",
    "        dataset = torch.utils.data.Subset(dataset, indices)\n",
    "    \n",
    "    train_size = int(train_split * dataset_size)\n",
    "    val_size = int(val_split * dataset_size)\n",
    "    test_size = dataset_size - train_size - val_size\n",
    "    \n",
    "    # Split the dataset\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size, test_size]\n",
    "    )\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# Usage example:\n",
    "# Assuming 'dataset' is your PyTorch dataset\n",
    "train_dataset, val_dataset, test_dataset = get_dataset_partitions_pytorch(dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffca7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming you have already created train_ds, val_ds, and test_ds\n",
    "\n",
    "# Define batch sizes\n",
    "batch_size = 64\n",
    "\n",
    "# Create DataLoader instances for train, val, and test datasets\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a4cf372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the desired image size\n",
    "IMAGE_SIZE = 224  # Example size, replace with your desired size\n",
    "\n",
    "# Define the sequence of transforms\n",
    "resize_and_rescale = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0/255.0, 1.0/255.0, 1.0/255.0])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31e701a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Create an instance of the model\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m model \u001b[38;5;241m=\u001b[39m CNNModel(n_classes)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Print model summary\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_classes' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=CHANNELS, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * (IMAGE_SIZE // 64) ** 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model = CNNModel(n_classes)\n",
    "\n",
    "# Print model summary\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988fac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0fed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Assuming you have already created train_ds as a PyTorch dataset\n",
    "\n",
    "# Define the sequence of data augmentation transforms for training\n",
    "data_augmentation = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=20),\n",
    "])\n",
    "\n",
    "# Create a DataLoader for the training dataset with data augmentation\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Apply data augmentation dynamically during training\n",
    "augmented_train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "for batch in augmented_train_loader:\n",
    "    images, labels = batch\n",
    "    images_augmented = torch.stack([data_augmentation(image) for image in images])\n",
    "    # Train your model using images_augmented and labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caac345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'train_dataset' is your PyTorch dataset\n",
    "# Define data augmentation transforms\n",
    "data_augmentation = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip the image horizontally with probability 0.5\n",
    "    transforms.RandomVerticalFlip(p=0.5),    # Randomly flip the image vertically with probability 0.5\n",
    "    transforms.RandomRotation(degrees=20),    # Randomly rotate the image by a maximum of 20 degrees\n",
    "])\n",
    "\n",
    "# Define a custom dataset class to apply data augmentation\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.dataset[index]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# Apply data augmentation to the training dataset\n",
    "train_dataset_augmented = CustomDataset(train_dataset, transform=data_augmentation)\n",
    "\n",
    "# Create a DataLoader with prefetching\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_augmented,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,  # You can adjust this based on your system\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cb5a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming you have defined your model and datasets\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the number of epochs\n",
    "epochs = 5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct_train += predicted.eq(targets).sum().item()\n",
    "        total_train += targets.size(0)\n",
    "        \n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_accuracy = correct_train / total_train\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444e4591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set the model to evaluation mode and move it to the device\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        # Move inputs and targets to the device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Get predictions\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        # Update total correct and total samples\n",
    "        total_correct += predicted.eq(targets).sum().item()\n",
    "        total_samples += targets.size(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = total_correct / total_samples\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ac472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores\n",
    "# Set the model to evaluation mode and move it to the device\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# List to store the predicted scores\n",
    "all_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in test_loader:\n",
    "        # Move inputs to the device\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Append the predicted scores\n",
    "        all_scores.append(outputs.cpu().numpy())  # Convert to numpy array and move to CPU\n",
    "\n",
    "# Concatenate the scores from all batches\n",
    "scores = np.concatenate(all_scores)\n",
    "\n",
    "# Now 'scores' contains the predicted scores for each class for each sample in the test dataset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
